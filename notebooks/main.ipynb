{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc667c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pathlib import Path\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5360da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "secret_key = os.getenv('HF_SECRET_KEY')\n",
    "secret_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=secret_key)\n",
    "cache_dir = Path(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77723cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", cache_dir=cache_dir/\"tokenizers\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", cache_dir=cache_dir/\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2bda99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df2d4664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,     40,   1205,    264,   3090,  16893]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"I need a water bottle\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "889d4853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,     40,   1205,    264,   3090,  16893,    369,    856,   8945,\n",
       "              0,   3639,    527,    279,   1888,  16097,    311,   1427,    704,\n",
       "            369,   5380,   4599,    499,   1176,   1212,    704,     11,    499,\n",
       "           1253,   2733,   1093,    701,   1716,    374,   2133,   1555,   1475,\n",
       "          10474,   3284,     13,   1472,    690,   1518,   1124,   3139,   1139,\n",
       "            872,   1866,    323,   1935,    389,    502,  12034,    304,   2204,\n",
       "           5627,    520]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate output\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    # do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8a0290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 56])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50e0aeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need a water bottle for my baby! What are the best brands to look out for?\n",
      "When you first start out, you may feel like your child is going through every phase possible. You will see them grow into their own and take on new interests in different ways at\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Decode and print the result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce7864d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4.60 GB\n",
      "Total number of parameters: 1.24 B\n"
     ]
    }
   ],
   "source": [
    "# Calculate the size of the model in bytes\n",
    "model_size = sum(p.element_size() * p.numel() for p in model.parameters())\n",
    "model_size_MB = model_size / (1024 ** 3)  # Convert to megabytes\n",
    "print(f\"Model size: {model_size_MB:.2f} GB\")\n",
    "\n",
    "\n",
    "# Count the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {(total_params / 1e9):.2f} B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c48db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
